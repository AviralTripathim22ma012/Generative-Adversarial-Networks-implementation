# -*- coding: utf-8 -*-
"""m22ma012_PA5_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dht5WcSdoq15pNNayfCm3WcsW1oOuLe3

**REFERENCES**
"""

'''
REFEREMCES:

for QUESTION 1:

https://www.youtube.com/watch?v=QE-AfZLVX2w
https://mafda.medium.com/gans-deep-convolutional-gans-with-mnist-part-3-8bad9a96ff65
https://stackoverflow.com/questions/68243122/runtimeerror-given-groups-1-weight-of-size-64-32-3-3-expected-input128
https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
https://github.com/mafda/generative_adversarial_networks_101/blob/master/src/mnist/01_GAN_MNIST.ipynb
https://github.com/coreprinciple6/GAN-Study/tree/master/DCGAN
https://github.com/AKASHKADEL/dcgan-mnist/tree/master/


for QUESTION 2:

@inproceedings{Karras2021,
  author = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\"ark\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title = {Alias-Free Generative Adversarial Networks},
  booktitle = {Proc. NeurIPS},
  year = {2021}
}


@inproceedings{Karras2020ada,
  title     = {Training Generative Adversarial Networks with Limited Data},
  author    = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc. NeurIPS},
  year      = {2020}
}


https://github.com/jeffheaton/stylegan2-toys/tree/main
https://github.com/jeffheaton/stylegan2-toys/blob/main/morph_video_real.ipynb
https://github.com/justinpinkney/stylegan-matlab-playground/blob/master/examples/interpolation.md
https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb
https://blog.paperspace.com/how-to-set-up-stylegan2-ada-pytorch-on-paperspace/
https://github.com/NVlabs/stylegan2-ada-pytorch#pretrained-networks
https://discuss.pystorch.org/t/running-a-pre-trained-tensorflow-stylegan-model-in-pytorch/42001
https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb


ADDITIONAL RESOURCES USED:

perplexity.ai
chat.openai.com
bing.com

'''

"""---



---

**QUESTION 1: TRAINING A DCGAN**

---



---
"""

import torch
from torchvision import datasets, transforms
import torchvision.utils as vutils

# Define the transformations to be applied to the dataset
transform = transforms.Compose([
    transforms.ToTensor(), # convert image to tensor
    transforms.Normalize((0.1307,), (0.3081,)) # normalize the image with mean and std
])

# Load the XMNIST dataset
train_dataset = datasets.EMNIST(root='./data', train=True, download=True, transform=transform, split= 'balanced')
test_dataset = datasets.EMNIST(root='./data', train=False, download=True, transform=transform, split= 'balanced')

# Create data loaders for the dataset
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)

print(type(test_loader))

import torch.nn as nn
import torch.nn.functional as F

class Generator(nn.Module):
    def __init__(self, nc, nz, ngf):
      super(Generator, self).__init__()
      self.network = nn.Sequential(
          nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias=False),
          nn.BatchNorm2d(ngf*4),
          nn.ReLU(True),

          nn.ConvTranspose2d(ngf*4, ngf*2, 3, 2, 1, bias=False),
          nn.BatchNorm2d(ngf*2),
          nn.ReLU(True),

          nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),
          nn.BatchNorm2d(ngf),
          nn.ReLU(True),

          nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
          nn.Tanh()
      )

    def forward(self, input):
      output = self.network(input)
      return output

import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models


class Discriminator(nn.Module):
    def __init__(self, nc, ndf):
        super(Discriminator, self).__init__()
        self.network = nn.Sequential(

                nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
                nn.LeakyReLU(0.2, inplace=True),

                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
                nn.BatchNorm2d(ndf * 2),
                nn.LeakyReLU(0.2, inplace=True),

                nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),
                nn.BatchNorm2d(ndf * 4),
                nn.LeakyReLU(0.2, inplace=True),

                nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),
                nn.Sigmoid()
            )
    def forward(self, input):
        output = self.network(input)
        return output.view(-1, 1).squeeze(1)

# generator = Generator(nc, nz, ngf)
# discriminator = Discriminator(nc, ndf)

import torch
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
import math
import itertools
import imageio
import natsort
from glob import glob

def generate_images(epoch, path, fixed_noise, num_test_samples, netG, device, use_fixed=False):
    z = torch.randn(num_test_samples, 100, 1, 1, device=device)
    size_figure_grid = int(math.sqrt(num_test_samples))
    title = None

    if use_fixed:
        generated_fake_images = netG(fixed_noise)
        path += 'fixed_noise/'
        title = 'Fixed Noise'
    else:
        generated_fake_images = netG(z)
        path += 'variable_noise/'
        title = 'Variable Noise'

    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6,6))
    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):
        ax[i,j].get_xaxis().set_visible(False)
        ax[i,j].get_yaxis().set_visible(False)
    for k in range(num_test_samples):
        i = k//4
        j = k%4
        ax[i,j].cla()
        ax[i,j].imshow(generated_fake_images[k].data.cpu().numpy().reshape(28,28), cmap='Greys')
    label = 'Epoch_{}'.format(epoch+1)
    fig.text(0.5, 0.04, label, ha='center')
    fig.suptitle(title)
    #fig.savefig(path+label+'.png')

def save_gif(path, fps, fixed_noise=False):
    if fixed_noise==True:
        path += 'fixed_noise/'
    else:
        path += 'variable_noise/'
    images = glob(path + '*.png')
    images = natsort.natsorted(images)
    gif = []

    for image in images:
        gif.append(imageio.imread(image))
    imageio.mimsave(path+'animated.gif', gif, fps=fps)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torch.autograd import Variable
from torchvision.utils import save_image

num_epochs= 100
ndf= 32
ngf= 32
nz= 100
d_lr= 0.0002
g_lr= 0.0002
nc= 1
num_test_samples= 16
output_path= "/contents/"
fps= 5
use_fixed= True

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using", device)

# Define Discriminator and Generator architectures
netG = Generator(nc, nz, ngf).to(device)
netD = Discriminator(nc, ndf).to(device)

# loss function
criterion = nn.BCELoss()

# optimizers
optimizerD = optim.Adam(netD.parameters(), lr= d_lr)
optimizerG = optim.Adam(netG.parameters(), lr= g_lr)

# initialize other variables
real_label = 1
fake_label = 0
num_batches = len(train_loader)
fixed_noise = torch.randn(num_test_samples, 100, 1, 1, device=device)

# Lists to keep track of progress
img_list = []
G_losses = []
D_losses = []
iters = 0

for epoch in range(num_epochs):
        for i, (real_images, _) in enumerate(train_loader):
            bs = real_images.shape[0]



            ##############################
            #   Training discriminator   #
            ##############################

            netD.zero_grad()
            real_images = real_images.to(device)
            label = torch.full((bs,), real_label, device=device)


            # real_images = torch.randn(128, 1, 28, 28)  # Example input data
            # new_images = torch.cat([real_images] * 3, dim=1)  # Replicate the single channel
            # output = netD(new_images.to(device))  # Pass the new tensor to the neural network


            output = netD(real_images)
            lossD_real = criterion(output, label.float())
            lossD_real.backward()
            D_x = output.mean().item()

            noise = torch.randn(bs, nz, 1, 1, device=device)
            fake_images = netG(noise)
            label.fill_(fake_label)
            output = netD(fake_images.detach())
            lossD_fake = criterion(output, label.float())
            lossD_fake.backward()
            D_G_z1 = output.mean().item()
            lossD = lossD_real + lossD_fake
            optimizerD.step()



            ##########################
            #   Training generator   #
            ##########################

            netG.zero_grad()
            label.fill_(real_label)
            output = netD(fake_images)
            lossG = criterion(output, label.float())
            lossG.backward()
            D_G_z2 = output.mean().item()
            optimizerG.step()

            if (i+1)%100 == 0:
                print('Epoch [{}/{}], step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, Discriminator - D(G(x)): {:.2f}, Generator - D(G(x)): {:.2f}'.format(epoch+1, num_epochs,
                                                            i+1, num_batches, lossD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))

            # Save Losses for plotting later
            G_losses.append(lossG.item())
            D_losses.append(lossD.item())


            # Check how the generator is doing by saving G's output on fixed_noise
            if (iters % 882 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):
                with torch.no_grad():
                    fake = netG(fixed_noise).detach().cpu()
                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

            iters += 1


        netG.eval()
        #generate_images(epoch, output_path, fixed_noise, num_test_samples, netG, device, use_fixed= use_fixed)
        netG.train()

generate_images(epoch, output_path, fixed_noise, num_test_samples, netG, device, use_fixed= use_fixed)

# # Save gif:
# save_gif(output_path, fps, fixed_noise= use_fixed)

plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses,label="G")
plt.plot(D_losses,label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Grab a batch of real images from the dataloader
real_batch = next(iter(train_loader))

# Plot the real images
plt.figure(figsize=(15,15))
plt.subplot(1,2,1)
plt.axis("off")
plt.title("Real Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:16], padding=5, normalize=True).cpu(),(1,2,0)))

# Plot the fake images from the last epoch
plt.subplot(1,2,2)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(np.transpose(img_list[-1],(1,2,0)))
plt.show()

im_1= plt.imshow(np.transpose(img_list[0], (1,2,0)))

im_50= plt.imshow(np.transpose(img_list[50], (1,2,0)))

im_100= plt.imshow(np.transpose(img_list[100], (1,2,0)))

"""---



---



---

**QUESTION 2: STYLE GAN**

---



---

**GENERATING 10 IMAGES FROM STYLE GAN 3**
"""

!git clone https://github.com/NVlabs/stylegan3.git
!pip install ninja

"""
Verify that StyleGAN has been cloned."""

!ls /content/stylegan3

# HIDE OUTPUT
URL = "https://api.ngc.nvidia.com/v2/models/nvidia/research/"\
      "stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl"

!python /content/stylegan3/gen_images.py \
    --network={URL} \
  --outdir=/content/results --seeds=0-9

"""We can now display the images created."""

!ls /content/results

import os
from PIL import Image

folder_path = "/content/results"
image_extensions = ["jpg", "jpeg", "png", "gif"] # Add any other image extensions you want to include

for filename in os.listdir(folder_path):
    extension = filename.split(".")[-1].lower()
    if extension in image_extensions:
        image_path = os.path.join(folder_path, filename)
        image = Image.open(image_path)
        image.show()

"""---



---

**INTERPOLATION BETWEEN 2 IMAGES**

---



---
"""

NETWORK = "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"

!wget http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2
!bzip2 -d shape_predictor_5_face_landmarks.dat.bz2

import sys
!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git
!pip install ninja
sys.path.insert(0, "/content/stylegan2-ada-pytorch")

import cv2
import numpy as np
from PIL import Image
import dlib

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_5_face_landmarks.dat')

def find_eyes(img):
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  rects = detector(gray, 0)

  if len(rects) == 0:
    raise ValueError("No faces detected")
  elif len(rects) > 1:
    raise ValueError("Multiple faces detected")

  shape = predictor(gray, rects[0])
  features = []

  for i in range(0, 5):
    features.append((i, (shape.part(i).x, shape.part(i).y)))

  return (int(features[3][1][0] + features[2][1][0]) // 2, \
    int(features[3][1][1] + features[2][1][1]) // 2), \
    (int(features[1][1][0] + features[0][1][0]) // 2, \
    int(features[1][1][1] + features[0][1][1]) // 2)

def crop_stylegan(img):
  left_eye, right_eye = find_eyes(img)
  d = abs(right_eye[0] - left_eye[0])
  z = 255/d
  ar = img.shape[0]/img.shape[1]
  w = img.shape[1] * z
  img2 = cv2.resize(img, (int(w), int(w*ar)))
  bordersize = 1024
  img3 = cv2.copyMakeBorder(
      img2,
      top=bordersize,
      bottom=bordersize,
      left=bordersize,
      right=bordersize,
      borderType=cv2.BORDER_REPLICATE)

  left_eye2, right_eye2 = find_eyes(img3)

  crop1 = left_eye2[0] - 385
  crop0 = left_eye2[1] - 490
  return img3[crop0:crop0+1024,crop1:crop1+1024]

def interpolate(img1, img2, video_name):

    from matplotlib import pyplot as plt
    import cv2
    import torch
    import dnnlib
    import legacy
    import PIL.Image
    import numpy as np
    import imageio
    from tqdm.notebook import tqdm
    from google.colab.patches import cv2_imshow

    image_source = cv2.imread(img1)

    image_target = cv2.imread(img2)

    cropped_source = crop_stylegan(image_source)
    cropped_target = crop_stylegan(image_target)


    cv2.imwrite("cropped_source.png", cropped_source)
    cv2.imwrite("cropped_target.png", cropped_target)

    cmd = f"python /content/stylegan2-ada-pytorch/projector.py --save-video 0 --num-steps 10 --outdir=out_source --target=cropped_source.png --network={NETWORK}"
    !{cmd}

    cmd = f"python /content/stylegan2-ada-pytorch/projector.py --save-video 0 --num-steps 10 --outdir=out_target --target=cropped_target.png --network={NETWORK}"
    !{cmd}


    STEPS = 70
    FPS = 20
    FREEZE_STEPS = 30

    lvec1 = np.load('/content/out_source/projected_w.npz')['w']
    lvec2 = np.load('/content/out_target/projected_w.npz')['w']

    network_pkl = "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
    device = torch.device('cuda')
    with dnnlib.util.open_url(network_pkl) as fp:
        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore

    diff = lvec2 - lvec1
    step = diff / STEPS
    current = lvec1.copy()
    target_uint8 = np.array([1024,1024,3], dtype=np.uint8)


    video = imageio.get_writer(f"/content/{video_name}.mp4", mode='I', fps=FPS, codec='libx264', bitrate='16M')

    for j in tqdm(range(STEPS)):

      z = torch.from_numpy(current).to(device)
      synth_image = G.synthesis(z, noise_mode='const')
      synth_image = (synth_image + 1) * (255/2)
      synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()

      repeat = FREEZE_STEPS if j==0 or j==(STEPS-1) else 1

      for i in range(repeat):
        video.append_data(synth_image)
      current = current + step


    video.close()


    # Open the video file
    video = cv2.VideoCapture(f"/content/{video_name}.mp4")

    # Initialize variables
    fps = video.get(cv2.CAP_PROP_FPS)
    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    frames_to_extract = [int(fps*i) for i in range(1, 7)]
    frames = []

    # Loop through each frame and extract the ones we want
    for i in range(frame_count):
        ret, frame = video.read()
        if i in frames_to_extract:
            frames.append(frame)

    # Concatenate the frames horizontally
    combined_image = np.concatenate(frames, axis=1)

    # Display the combined image
    return cv2_imshow(combined_image)

me= "/content/me.jpg"
akash= "/content/akash.jpg"

interpolate(me, akash, video_name= 'video1')

me= "/content/me.jpg"
sanjeev= "/content/sanjeev.jpg"

interpolate(me, sanjeev, video_name= 'video2')

me= "/content/me.jpg"
tanmay= "/content/tanmay.jpg"

interpolate(me, tanmay, video_name= 'video3')

me= "/content/me.jpg"
alok= "/content/alok.jpg"

interpolate(me, alok, video_name= 'video4')

me= "/content/me.jpg"
girish= "/content/girish.jpg"

interpolate(me, girish, video_name= 'video5')
